SHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(SHIlm)
#shifting regression
SSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SSHIlm)
##########################################################
##                      CLEAN                           ##
##########################################################
#remove participants with missing data
preclean <- na.omit(preclean)
#ensure all numeric columns are numeric
preclean$Q17 <- as.numeric(preclean$Q17)
preclean$QID19 <- as.numeric(preclean$QID19)
preclean$Q2 <- as.numeric(preclean$Q2)
preclean$Q4 <- as.numeric(preclean$Q4)
preclean$Q5 <- as.numeric(preclean$Q5)
preclean$Q7 <- as.numeric(preclean$Q7)
preclean$Q8 <- as.numeric(preclean$Q8)
## REMOVE FRAUD RESPONSES ##
############################
#remove participants with reaction times lower than 150ms
cleandf <- subset(preclean, speed > 150)
cleandf <- subset(cleandf, congruent > 150)
cleandf <- subset(cleandf, incongruent > 150)
#remove participants with SR under 500
cleandf <- subset(cleandf, Q7 > 500)
#remove participants with weekly play time more than total
cleandf <- subset(cleandf, Q4 > Q5)
#remove participants with current SR higher than peak SR
cleandf <- subset(cleandf, Q7 >= Q8)
#remove responses indicating having started playing overwatch 7 or more years ago (the game is less than 6 years old)
cleandf <- subset(cleandf, (Q17 - Q2) <= 6)
#rename columns
cleandf <- cleandf %>%
rename(
gender = Q22,
age = Q17,
country = Q20,
race = Q26,
education = Q24,
SES = QID19,
ageoverwatch = Q2,
accounts = Q3,
totalhours = Q4,
weeklyhours = Q5,
class = Q6,
peaksr = Q7,
currentsr = Q8,
team = Q9,
aids = Q10,
level = Q16,
league = Q17.1
)
View(cleandf)
##########################################################
##                      CLEAN                           ##
##########################################################
#remove participants with missing data
preclean <- na.omit(preclean)
#ensure all numeric columns are numeric
preclean$Q17 <- as.numeric(preclean$Q17)
preclean$QID19 <- as.numeric(preclean$QID19)
preclean$Q2 <- as.numeric(preclean$Q2)
preclean$Q4 <- as.numeric(preclean$Q4)
preclean$Q5 <- as.numeric(preclean$Q5)
preclean$Q7 <- as.numeric(preclean$Q7)
preclean$Q8 <- as.numeric(preclean$Q8)
## REMOVE FRAUD RESPONSES ##
############################
#remove participants with reaction times lower than 150ms
cleandf <- subset(preclean, speed > 150)
cleandf <- subset(cleandf, congruent > 150)
cleandf <- subset(cleandf, incongruent > 150)
#remove participants with SR under 500
cleandf <- subset(cleandf, Q7 > 500)
#remove participants with weekly play time more than total
cleandf <- subset(cleandf, Q4 > Q5)
#remove participants with current SR higher than peak SR
cleandf <- subset(cleandf, Q7 >= Q8)
#remove responses indicating having started playing overwatch 7 or more years ago (the game is less than 6 years old)
cleandf <- subset(cleandf, (Q17 - Q2) <= 6)
#rename columns
cleandf <- cleandf %>%
rename(
gender = Q22,
age = Q17,
country = Q20,
race = Q26,
education = Q24,
SES = QID19,
ageoverwatch = Q2,
accounts = Q3,
totalhours = Q4,
weeklyhours = Q5,
class = Q6,
peaksr = Q7,
currentsr = Q8,
team = Q9,
aids = Q10,
level = Q16,
league = Q17.1
)
#remove specifically examined participants (removing participants with periods of rapid responses less than 50ms) and remove missing data (i.e. prefer not to say) - for loop does not work
cleandf <- subset(cleandf, userCode != 137329)
cleandf <- subset(cleandf, userCode != 138216)
cleandf <- subset(cleandf, userCode != 138241)
cleandf <- subset(cleandf, userCode != 138290)
cleandf <- subset(cleandf, userCode != 138368)
cleandf <- subset(cleandf, userCode != 138380)
cleandf <- subset(cleandf, userCode != 138520)
cleandf <- subset(cleandf, userCode != 138563)
cleandf <- subset(cleandf, userCode != 138694)
cleandf <- subset(cleandf, userCode != 138695)
cleandf <- subset(cleandf, userCode != 139371)
#calculate mahalanobis distance
MD <- mahalanobis(cleandf[, c(22, 25, 27)], colMeans(cleandf[,c(22, 25, 27)]), cov(cleandf[, c(22, 25, 27)]))
#add mahalanobis distance to the data frame
cleandf$MD <- round(MD, 3)
#set threshhold to 10
cleandf$outlier_maha <- FALSE
cleandf$outlier_maha[cleandf$MD > 10] <- TRUE
#remove outliers
cleandf <- subset(cleandf, outlier_maha != TRUE)
#rank order education
cleandf$educationcode <- as.numeric(factor(cleandf$education, levels = c("No schooling completed", "Less than a high school diploma (GCSEs)", "High school degree or equivalent (A levels)", "Bachelor's degree", "Master's degree", "Doctorate degree")))
stat.desc(cleandf)
#correlation matrix
cormat <- cleandf %>% select(3, 8, 11, 12, 14, 15, 22, 25, 27, 30)
source("http://www.sthda.com/upload/rquery_cormat.r")
rquery.cormat(cormat, type="upper")
##########################################################
##                      ANALYSIS                        ##
##########################################################
#descriptive statistics
stat.desc(cleandf)
table(cleandf$gender)
table(cleandf$country)
table(cleandf$race)
table(cleandf$education)
#correlation matrix
cormat <- cleandf %>% select(3, 8, 11, 12, 14, 15, 22, 25, 27, 30)
source("http://www.sthda.com/upload/rquery_cormat.r")
rquery.cormat(cormat, type="upper")
#z transform predictor variables
cleandf$zCSR <- (cleandf$currentsr - mean(cleandf$currentsr))/sd(cleandf$currentsr)
cleandf$zAGE <- (cleandf$age - mean(cleandf$age))/sd(cleandf$age)
cleandf$zSES <- (cleandf$SES - mean(cleandf$SES))/sd(cleandf$SES)
cleandf$zEDU <- (cleandf$educationcode - mean(cleandf$educationcode))/sd(cleandf$educationcode)
cleandf$zTH <- (cleandf$totalhours - mean(cleandf$totalhours))/sd(cleandf$totalhours)
cleandf$zWH <- (cleandf$weeklyhours - mean(cleandf$weeklyhours))/sd(cleandf$weeklyhours)
#proficiency evidence regression
PROFlm <- lm(currentsr ~ zTH + level + league, data=cleandf)
summary(PROFlm)
## CURRENT SR ##
################
#updating regression
UPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(UPDlm)
#inhibition regression
INHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(INHlm)
#shifting regression
SHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(SHIlm)
## TOTAL HOURS ##
#################
#updating regression
THUPDlm <- lm(updatingaccuracy ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THUPDlm)
#inhibition regression
THINHlm <- lm(RTinterference ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THINHlm)
#shifting regression
THSHIlm <- lm(switchCosts ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THSHIlm)
#subset different classes
tankdf <- subset(cleandf, class == "Tank")
supportdf <- subset(cleandf, class == "Support")
damagedf <- subset(cleandf, class == "Damage")
## DAMAGE ##
############
#updating regression
DUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DUPDlm)
#inhibition regression
DINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DINHlm)
#shifting regression
DSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DSHIlm)
## SUPPORT ##
#############
#updating regression
SUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SUPDlm)
#inhibition regression
SINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SINHlm)
#shifting regression
SSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SSHIlm)
## TANK ##
##########
#updating regression
TUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TUPDlm)
#inhibition regression
TINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TINHlm)
#shifting regression
TSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TSHIlm)
#updating regression
UPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(UPDlm)
#inhibition regression
INHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(INHlm)
#shifting regression
SHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(SHIlm)
updating regression
THUPDlm <- lm(updatingaccuracy ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THUPDlm)
#inhibition regression
THINHlm <- lm(RTinterference ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THINHlm)
THSHIlm <- lm(switchCosts ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THSHIlm)
DUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DUPDlm)
DINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DINHlm)
#shifting regression
DSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DSHIlm)
SUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SUPDlm)
#inhibition regression
SINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SINHlm)
SSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SSHIlm)
#updating regression
TUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TUPDlm)
TINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TINHlm)
TSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TSHIlm)
stat.desc(cleandf)
rquery.cormat(cormat, type="upper")
stat.desc(cleandf)
PROFlm <- lm(currentsr ~ zTH + level + league, data=cleandf)
summary(PROFlm)
UPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(UPDlm)
#inhibition regression
INHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(INHlm)
#shifting regression
SHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=cleandf)
summary(SHIlm)
#updating regression
THUPDlm <- lm(updatingaccuracy ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THUPDlm)
THINHlm <- lm(RTinterference ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THINHlm)
#shifting regression
THSHIlm <- lm(switchCosts ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THSHIlm)
SUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SUPDlm)
SINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SINHlm)
SSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SSHIlm)
#updating regression
DUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DUPDlm)
#inhibition regression
DINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DINHlm)
#shifting regression
DSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DSHIlm)
#updating regression
SUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SUPDlm)
summary(SUPDlm)
#inhibition regression
SINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SINHlm)
#shifting regression
SSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=supportdf)
summary(SSHIlm)
TUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TUPDlm)
#inhibition regression
TINHlm <- lm(RTinterference ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TINHlm)
#shifting regression
TSHIlm <- lm(switchCosts ~ zCSR + zAGE + zSES + zEDU, data=tankdf)
summary(TSHIlm)
#updating regression
THUPDlm <- lm(updatingaccuracy ~ zTH + zAGE + zSES + zEDU, data=cleandf)
summary(THUPDlm)
#write cleandf
write.table(cleandf, file = paste(outputFolder, "cleanDataFrame", sep = "/"), sep = ",", row.names = FALSE)
#write cleandf
write.table(cleandf, file = paste(outputFolder, "cleanDataFrame.csv", sep = "/"), sep = ",", row.names = FALSE)
#write cleandf
write.table(cleandf, file = paste(outputFolder, "cleanData.csv", sep = "/"), sep = ",", row.names = FALSE)
tankdf <- subset(cleandf, class == "Tank")
write.table(tankdf, file = paste(outputFolder, "tankData.csv", sep = "/"), sep = ",", row.names = FALSE)
supportdf <- subset(cleandf, class == "Support")
write.table(supportdf, file = paste(outputFolder, "supportData.csv", sep = "/"), sep = ",", row.names = FALSE)
damagedf <- subset(cleandf, class == "Damage")
write.table(damagedf, file = paste(outputFolder, "damageData.csv", sep = "/"), sep = ",", row.names = FALSE)
summary(PROFlm)
write.csv(PROFlm, file = paste(outputFolder, "proflm.csv", sep = "/"), sep = ",", row.names = FALSE)
write.csv(PROFlm, file = paste(outputFolder, "proflm.csv", sep = "/"), row.names = FALSE)
write.csv(PROFlm, file = paste(outputFolder, "proflm.csv"), row.names = FALSE)
summ(PROFlm)
export_summary(UPDlm)
export_summs(UPDlm)
install.packages(jtools)
install.packages("jplot")
install.packages("jtools")
library(jtools)
summ(PROFlm)
library(pastecs)
library(jtools)
summ(PROFlm)
summary(PROFlm)
export_summs(PROFlm)
install.packages("huxtable")
library(huxtable)
export_summs(PROFlm)
export_summs(PROFlm, scale = TRUE)
export_summs(PROFlm, model.info = FALSE, model.fit = FALSE)
out <- capture.output(summary(PROFlm))
out <- capture.output(summary(PROFlm))
cat("PROFlm", out, file = "PROFlm.txt", sep="n", append=TRUE)
install.packages("stargazer")
library(stargazer)
stargazer(PROFlm, type="html", out = "PROFlm")
stargazer(PROFlm, out = "PROFlm")
stargazer(PROFlm, type="text" out = "PROFlm")
stargazer(PROFlm, type="text", out = "PROFlm")
stargazer(PROFlm, type="text", out = "output/PROFlm")
jpeg('rplot.jpg')
plot(PROFlm)
dev.off()
jpeg('rplot.jpg')
plot(summary(PROFlm))
dev.off()
library(tidyverse)
library(corrplot)
PROFlm <- lm(currentsr ~ zTH + level + league, data=cleandf)
dev.copy(png, 'PROFlm.png')
dev.off()
PROFlm <- lm(currentsr ~ zTH + level + league, data=cleandf)
dev.copy(png, 'PROFlm.png')
dev.off()
View(cleandf)
#updating regression
DUPDlm <- lm(updatingaccuracy ~ zCSR + zAGE + zSES + zEDU, data=damagedf)
summary(DUPDlm)
setwd("C:/Users/monty/OneDrive/Desktop/Masters/github repos/Cognitive-Research-Project")
##########################################################
##                      PRELIMINARIES                   ##
##########################################################
set.seed(1234321)
library(tidyverse)
library(corrplot)
library(pastecs)
# clear workspace
rm(list=ls())
##########################################################
##                  SET WORK ENVIRONMENT                ##
##########################################################
# set general folder
workspace = getwd()
# set name of script with functions
funScript = "overwatch_funs.R"
# load functions into workspace
source(paste(workspace, funScript, sep = "/"))
# set names of folders for input and output files, set output filename
inputFolder = paste(workspace, "input", sep = "/")
outputFolder = paste(workspace, "output", sep = "/")
# set data input folder as default location of input files
setwd(inputFolder)
##########################################################
##                        PREPARE                       ##
##########################################################
# unzip and merge all data
source(paste(workspace, "overwatch_01_merge.R", sep = "/"))
# aggregate data
source(paste(workspace, "overwatch_02_aggregate.R", sep = "/"))
#combine questionnaire and task data
source(paste(workspace, "overwatch_03_combine.R", sep = "/"))
#clean dataframe
source(paste(workspace, "overwatch_04_clean.R", sep = "/"))
#run analysis
source(paste(workspace, "overwatch_05_analysis.R", sep = "/"))
##########################################################
##                ANALYSIS SUMMARIES                    ##
##########################################################
#proficiency evidence regression
summary(PROFlm)
#regressions for whole sample
##  CURRENT SR  ##
##################
#updating regression
summary(UPDlm)
#inhibition regression
summary(INHlm)
#shifting regression
summary(SHIlm)
## TOTAL HOURS ##
#################
#updating regression
summary(THUPDlm)
#inhibition regression
summary(THINHlm)
#shifting regression
summary(THSHIlm)
#regressions for separate classes
## DAMAGE ##
############
#updating regression
summary(DUPDlm)
#inhibition regression
summary(DINHlm)
#shifting regression
summary(DSHIlm)
## SUPPORT ##
#############
#updating regression
summary(SUPDlm)
#inhibition regression
summary(SINHlm)
#shifting regression
summary(SSHIlm)
## TANK ##
##########
#updating regression
summary(TUPDlm)
#inhibition regression
summary(TINHlm)
#shifting regression
summary(TSHIlm)
View(preclean)
rawq = read.csv("questionnaire/overwatch_project.csv")
#delete unneeded columns
cleanq <- rawq %>% select(7, 18:32, 58:60)
#delete unneeded rows
cleanq <- cleanq[-c(1, 2),]
#delete incomplete forms
cleanq <- cleanq[!(cleanq$Finished == FALSE),]
#rename sessiontoken to match aggShiftingData
colnames(cleanq)[colnames(cleanq) == "sessiontoken"] <- "sessionToken"
#combine data from questionnaires and tasks
df <- merge(cleanq, aggInhibitionData, by = "sessionToken", all = TRUE)
df1 <- merge(df, aggShiftingData, by = "sessionToken", all = TRUE)
preclean <- merge(df1, aggUpdatingData, by = "sessionToken", all = TRUE)
preclean <- preclean[, !(names(preclean) %in% c("userCode.y", "userCode.x"))]
preclean <- na.omit(preclean)
#ensure all numeric columns are numeric
preclean$Q17 <- as.numeric(preclean$Q17)
preclean$QID19 <- as.numeric(preclean$QID19)
preclean$Q2 <- as.numeric(preclean$Q2)
preclean$Q4 <- as.numeric(preclean$Q4)
preclean$Q5 <- as.numeric(preclean$Q5)
preclean$Q7 <- as.numeric(preclean$Q7)
preclean$Q8 <- as.numeric(preclean$Q8)
#remove participants with reaction times lower than 150ms
cleandf <- subset(preclean, speed > 150)
cleandf <- subset(cleandf, congruent > 150)
cleandf <- subset(cleandf, incongruent > 150)
#remove participants with SR under 500
cleandf <- subset(cleandf, Q7 > 500)
#remove participants with weekly play time more than total
cleandf <- subset(cleandf, Q4 > Q5)
#remove participants with current SR higher than peak SR
cleandf <- subset(cleandf, Q7 >= Q8)
#remove responses indicating having started playing overwatch 7 or more years ago (the game is less than 6 years old)
cleandf <- subset(cleandf, (Q17 - Q2) <= 6)
#remove specifically examined participants (removing participants with periods of rapid responses less than 50ms) and remove missing data (i.e. prefer not to say) - for loop does not work
cleandf <- subset(cleandf, userCode != 137329)
cleandf <- subset(cleandf, userCode != 138216)
cleandf <- subset(cleandf, userCode != 138241)
cleandf <- subset(cleandf, userCode != 138290)
cleandf <- subset(cleandf, userCode != 138368)
cleandf <- subset(cleandf, userCode != 138380)
cleandf <- subset(cleandf, userCode != 138520)
cleandf <- subset(cleandf, userCode != 138563)
cleandf <- subset(cleandf, userCode != 138694)
cleandf <- subset(cleandf, userCode != 138695)
cleandf <- subset(cleandf, userCode != 139371)
#calculate mahalanobis distance
MD <- mahalanobis(cleandf[, c(22, 25, 27)], colMeans(cleandf[,c(22, 25, 27)]), cov(cleandf[, c(22, 25, 27)]))
#add mahalanobis distance to the data frame
cleandf$MD <- round(MD, 3)
#set threshhold to 10
cleandf$outlier_maha <- FALSE
cleandf$outlier_maha[cleandf$MD > 10] <- TRUE
#remove outliers
cleandf <- subset(cleandf, outlier_maha != TRUE)
#rank order education
cleandf$educationcode <- as.numeric(factor(cleandf$education, levels = c("No schooling completed", "Less than a high school diploma (GCSEs)", "High school degree or equivalent (A levels)", "Bachelor's degree", "Master's degree", "Doctorate degree")))
